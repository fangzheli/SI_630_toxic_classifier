{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a060ec61-b01e-4dae-b6d4-f11882b45da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"hw4_train.csv\")\n",
    "dev = pd.read_csv(\"hw4_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4e75ae-87ac-47c1-ba60-6f5122323088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = train[\"comment_text\"].tolist(), train[\"toxic\"].tolist()\n",
    "test_texts, test_labels = dev[\"comment_text\"].tolist(), dev[\"toxic\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3522b85-d0ff-45ad-bd02-1727ba204757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "max_length = 512\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=\"max_length\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7621b3-83ba-4dfb-802b-8f387d5448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelpfulnessDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HelpfulnessDataset(train_encodings, train_labels)\n",
    "test_dataset = HelpfulnessDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd2d5e-4b63-4498-ac2d-e555c5e66bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = np.round_(pred.predictions)\n",
    "  # preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  f1 = f1_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "      'f1'\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d7081-b278-4660-aa59-66445b5d6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = torch.nn.MSELoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ff90d-facf-4220-8512-e0a906088cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/Multilingual-MiniLM-L12-H384\", num_labels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227259be-740e-4e56-8700-fafb78400a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.03,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=80,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b39cde-bc74-43c1-99ba-e5d61450cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253dc7d-1cea-44c9-b308-b1aa14f7465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
